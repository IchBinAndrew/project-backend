import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
import numpy as np
import os
os.environ["WANDB_DISABLED"] = "true"

labels_map = {
    0: "üåû –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π",
    1: "üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π",
    2: "üå©Ô∏è –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π",
    3: "ü§® –°–∞—Ä–∫–∞—Å—Ç–∏—á–Ω—ã–π"
}

data = [
    {"text": "–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω. –ö–∞–º–µ—Ä–∞, –∫–æ–Ω–µ—á–Ω–æ, –∫–∞–∫ —É –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–∫–∏, –Ω–æ –∑–∞—Ç–æ –∫–æ—Ä–ø—É—Å –±–ª–µ—Å—Ç–∏—Ç.", "label": 3},
    {"text": "–û—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –ø–æ–∫—É–ø–∫–æ–π! –í—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ.", "label": 0},
    {"text": "–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ —É–∂–∞—Å. –ë–∞—Ç–∞—Ä–µ—è –Ω–µ –¥–µ—Ä–∂–∏—Ç, –≥—Ä–µ–µ—Ç—Å—è —Å–∏–ª—å–Ω–æ.", "label": 2},
    {"text": "–ù–æ—Ä–º–∞–ª—å–Ω–æ, —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏ –æ–ø—Ä–∞–≤–¥—ã–≤–∞–µ—Ç.", "label": 1},
    {"text": "–û, –∫–∞–∫–∞—è –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å‚Ä¶ –≤ 2007 –≥–æ–¥—É, –Ω–∞–≤–µ—Ä–Ω–æ–µ.", "label": 3},
    {"text": "–ö—É–ø–∏–ª –∏ –Ω–µ –ø–æ–∂–∞–ª–µ–ª. –í—Å—ë –±—ã—Å—Ç—Ä–æ –∏ —É–¥–æ–±–Ω–æ.", "label": 0},
    {"text": "–ù—É –¥–∞, —Ä–∞–±–æ—Ç–∞–µ—Ç. –ö–∞–∫ –∏ –º–æ–π —É—Ç—é–≥.", "label": 3},
    {"text": "–¢–µ–ª–µ—Ñ–æ–Ω –Ω–∏ –æ —á—ë–º. –ó–∞—Ä—è–¥–∫–∞ –º–µ–¥–ª–µ–Ω–Ω–∞—è, —ç–∫—Ä–∞–Ω —Ç—É—Å–∫–ª—ã–π.", "label": 2},
    {"text": "–°—Ä–µ–¥–Ω–∏–π –∞–ø–ø–∞—Ä–∞—Ç. –ù–µ –ª—É—á—à–µ –∏ –Ω–µ —Ö—É–∂–µ –¥—Ä—É–≥–∏—Ö.", "label": 1},
    {"text": "–ü—Ä–æ—Å—Ç–æ –±–æ–º–±–∞! –ö–∞–º–µ—Ä–∞ üî•", "label": 0},
]
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)
train_ds = Dataset.from_list(train_data)
val_ds = Dataset.from_list(val_data)

model_name = "cointegrated/rubert-tiny2"

tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_ds = train_ds.map(tokenize, batched=True)
val_ds = val_ds.map(tokenize, batched=True)

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)

training_args = TrainingArguments(
    output_dir="./model_output",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    save_total_limit=1,
    save_steps=500,
    logging_steps=100
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = np.mean(preds == labels)
    return {"accuracy": acc}
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=1).numpy()
        pred = np.argmax(probs)
    return labels_map[pred]
#-----------
print(predict_sentiment("–¢–µ–ª–µ—Ñ–æ–Ω —Ö–æ—Ä–æ—à–∏–π, –Ω–æ —á—É–≤—Å—Ç–≤—É–µ—Ç—Å—è –¥—É—Ö 2010 –≥–æ–¥–∞."))
